{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alcholic Anonymous News articles extraction from The Daily Star. Data Extraction of following parameters\n",
    "- Headline\n",
    "- Description\n",
    "- Author\n",
    "- Published_Date\n",
    "- News\n",
    "- URL\n",
    "- Keywords\n",
    "- Summary\n",
    "\n",
    "### Importing the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options # enables options in web browser\n",
    "from selenium import webdriver # web-based automation tool for Python\n",
    "from newspaper import Article # Article scraping & curation\n",
    "from bs4 import BeautifulSoup # Python library for pulling data out of HTML and XML files\n",
    "from requests import get # standard for making HTTP requests in Python\n",
    "import pandas as pd # library written for data manipulation and analysis\n",
    "import sys, time #  System-specific parameters and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Empty lists for Alcholic Anonymous News Articles parameters data to be extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines, descriptions, dates, authors, news, keywords, summaries, urls = [], [], [], [], [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the total no.of.pages by total no.of articles from google search resultsÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'Alcoholics Anonymous site:www.thedailystar.net'\n",
    "\n",
    "url = 'https://www.google.com/search?q=' + '+'.join(keyword.split())\n",
    "\n",
    "options = Options()\n",
    "options.headless = True\n",
    "browser = webdriver.Chrome(options=options)\n",
    "browser.get(url)\n",
    "\n",
    "page = browser.page_source\n",
    "soup = BeautifulSoup(page, 'lxml')\n",
    "max_pages = round([int(s) for s in soup.select_one('#resultStats').text.split() if s.isdigit()][0]/10)\n",
    "max_pages = max_pages + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterates max_pages value through while loop. Scraping the Articles urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 : 7\r"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        index +=1\n",
    "        page = browser.page_source\n",
    "        soup = BeautifulSoup(page, 'lxml')\n",
    "        linky = [soup.select('.r')[i].a['href'] for i in range(len(soup.select('.r')))]\n",
    "        urls.extend(linky)\n",
    "        if index == max_pages:\n",
    "            break\n",
    "        browser.find_element_by_xpath('//*[@id=\"pnnext\"]/span[2]').click()\n",
    "        time.sleep(2)\n",
    "        sys.stdout.write('\\r' + str(index) + ' : ' + str(max_pages) + '\\r')\n",
    "        sys.stdout.flush()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To remove duplicates urls entries in the list by executing below line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "urls = list(dict.fromkeys(urls))\n",
    "print(len(urls), type(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterates urls through for loop. Scraping the Articles with above parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 44sedailystar.net/city/news/body-8-year-old-found-demra-mosque-1727266hingyas-refugees-bangladesh-1684120oting-1546153\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for index, url in enumerate(urls):\n",
    "    try:\n",
    "        # Parse the url to NewsPlease \n",
    "        soup = BeautifulSoup(get(url).text, 'lxml')\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        article.nlp()\n",
    "        \n",
    "        # Extracts the Headlines\n",
    "        try:\n",
    "            try:\n",
    "                headlines.append(soup.select_one('meta[property=\"og:title\"]')['content'].strip())\n",
    "            except:\n",
    "                headlines.append(article.title.strip())\n",
    "        except:\n",
    "            headlines.append(None)\n",
    "            \n",
    "        # Extracts the Descriptions    \n",
    "        try:\n",
    "            try:\n",
    "                descriptions.append(soup.select_one('meta[property=\"og:description\"]')['content'].strip().replace('\\n', ' '))\n",
    "            except:\n",
    "                descriptions.append(article.meta_description.strip())\n",
    "        except:\n",
    "            descriptions.append(None)\n",
    "            \n",
    "        # Extracts the Authors\n",
    "        try:\n",
    "            try:\n",
    "                authors.append(soup.select_one('.detailed-content').select_one('span[itemprop=\"name\"]').a.text.strip())\n",
    "            except:\n",
    "                authors.append(article.authors.strip())\n",
    "        except:\n",
    "            authors.append(None)\n",
    "        \n",
    "        # Extracts the published dates\n",
    "        try:\n",
    "            try:\n",
    "                dates.append(soup.select_one('meta[property=\"article:published_time\"]')['content'].strip())\n",
    "            except:\n",
    "                dates.append(str(article.publish_date))\n",
    "        except:\n",
    "            dates.append(None)\n",
    "            \n",
    "        # Extracts the news articles\n",
    "        try:\n",
    "            try:\n",
    "                news.append(soup.select_one('.node-content').text.replace('\\n', '').strip())\n",
    "            except:\n",
    "                news.append(article.text.strip())\n",
    "        except:\n",
    "            news.append(None)\n",
    "            \n",
    "        # Extracts Keywords and Summaries    \n",
    "        try:\n",
    "            keywords.append(article.keywords)\n",
    "            summaries.append(article.summary)\n",
    "        except:\n",
    "            keywords.append(None)\n",
    "            summaries.append(None)\n",
    "            \n",
    "    except:\n",
    "        headlines.append(None)\n",
    "        descriptions.append(None)\n",
    "        authors.append(None)\n",
    "        dates.append(None)\n",
    "        news.append(None)\n",
    "        keywords.append(None)\n",
    "        summaries.append(None)\n",
    "\n",
    "    sys.stdout.write('\\r' + str(index) + ' : ' + str(url) + '\\r')\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Array Length of each list to create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 63 63 63 63 63 63 63\n"
     ]
    }
   ],
   "source": [
    "print(len(headlines), len(descriptions), len(authors), len(dates), len(news), len(keywords), len(summaries), len(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a csv file after checking array length and droping the missing values from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headlines</th>\n",
       "      <th>Descriptions</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Published_Dates</th>\n",
       "      <th>Articles</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Serenity in the South China Sea</td>\n",
       "      <td>The central, painful reality that the US must ...</td>\n",
       "      <td>Gareth Evans</td>\n",
       "      <td>2015-07-03T00:00:00+06:00</td>\n",
       "      <td>Diplomats and alcoholics don't always have as ...</td>\n",
       "      <td>[right, sea, international, china, zone, milit...</td>\n",
       "      <td>The dramatic build-up of China's military (esp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UN chief accuses India of intolerance with gay...</td>\n",
       "      <td>United Nations Chief Ban Ki-moon has accused I...</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-01-14T00:00:46+06:00</td>\n",
       "      <td>United Nations Chief Ban Ki-moon has accused I...</td>\n",
       "      <td>[india, state, accuses, centres, tawadkar, chi...</td>\n",
       "      <td>United Nations Chief Ban Ki-moon has accused I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>India ruling party minister slammed over plans...</td>\n",
       "      <td>Indian Prime Minister Narendra Modi's ruling p...</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-01-14T00:00:52+06:00</td>\n",
       "      <td>Indian Prime Minister Narendra Modi's ruling p...</td>\n",
       "      <td>[india, state, modis, goa, gays, centres, tawa...</td>\n",
       "      <td>Indian Prime Minister Narendra Modi's ruling p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ONLINE VOICES</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>2011-08-05T00:00:00+06:00</td>\n",
       "      <td>New International AirportGovt eyes Arial Beel,...</td>\n",
       "      <td>[international, priority, sectors, country, po...</td>\n",
       "      <td>AnonymousAll national economic and social sect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why wait for others when we can do it ourselves?</td>\n",
       "      <td>While some of us sit and wait for the world to...</td>\n",
       "      <td>Sihinta Sabeen Shembil</td>\n",
       "      <td>2018-11-16T00:00:00+06:00</td>\n",
       "      <td>While some of us sit and wait for the world to...</td>\n",
       "      <td>[parking, learning, garage, school, change, si...</td>\n",
       "      <td>While some of us sit and wait for the world to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Headlines  \\\n",
       "0                    Serenity in the South China Sea   \n",
       "1  UN chief accuses India of intolerance with gay...   \n",
       "2  India ruling party minister slammed over plans...   \n",
       "3                                      ONLINE VOICES   \n",
       "4   Why wait for others when we can do it ourselves?   \n",
       "\n",
       "                                        Descriptions                 Authors  \\\n",
       "0  The central, painful reality that the US must ...            Gareth Evans   \n",
       "1  United Nations Chief Ban Ki-moon has accused I...                    None   \n",
       "2  Indian Prime Minister Narendra Modi's ruling p...                    None   \n",
       "3                                                                       None   \n",
       "4  While some of us sit and wait for the world to...  Sihinta Sabeen Shembil   \n",
       "\n",
       "             Published_Dates  \\\n",
       "0  2015-07-03T00:00:00+06:00   \n",
       "1  2015-01-14T00:00:46+06:00   \n",
       "2  2015-01-14T00:00:52+06:00   \n",
       "3  2011-08-05T00:00:00+06:00   \n",
       "4  2018-11-16T00:00:00+06:00   \n",
       "\n",
       "                                            Articles  \\\n",
       "0  Diplomats and alcoholics don't always have as ...   \n",
       "1  United Nations Chief Ban Ki-moon has accused I...   \n",
       "2  Indian Prime Minister Narendra Modi's ruling p...   \n",
       "3  New International AirportGovt eyes Arial Beel,...   \n",
       "4  While some of us sit and wait for the world to...   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  [right, sea, international, china, zone, milit...   \n",
       "1  [india, state, accuses, centres, tawadkar, chi...   \n",
       "2  [india, state, modis, goa, gays, centres, tawa...   \n",
       "3  [international, priority, sectors, country, po...   \n",
       "4  [parking, learning, garage, school, change, si...   \n",
       "\n",
       "                                           Summaries  \n",
       "0  The dramatic build-up of China's military (esp...  \n",
       "1  United Nations Chief Ban Ki-moon has accused I...  \n",
       "2  Indian Prime Minister Narendra Modi's ruling p...  \n",
       "3  AnonymousAll national economic and social sect...  \n",
       "4  While some of us sit and wait for the world to...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl = pd.DataFrame({'Headlines' : headlines,\n",
    "                    'Descriptions' : descriptions,\n",
    "                    'Authors' : authors,\n",
    "                    'Published_Dates' : dates, \n",
    "                    'Articles' : news,\n",
    "                    'Keywords' : keywords,\n",
    "                    'Summaries' : summaries,})\n",
    "#tbl = tbl.dropna()\n",
    "tbl.to_csv('The_Daily_Star.csv', index=False)\n",
    "tbl.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
