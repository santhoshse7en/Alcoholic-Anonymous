{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alcholic Anonymous News articles extraction from India Today. Data Extraction of following parameters\n",
    "- Headline\n",
    "- Description\n",
    "- Author\n",
    "- Published_Date\n",
    "- News\n",
    "- URL\n",
    "- Keywords\n",
    "- Summary\n",
    "\n",
    "### Importing the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options # enables options in web browser\n",
    "from selenium import webdriver # web-based automation tool for Python\n",
    "from newspaper import Article # Article scraping & curation\n",
    "from bs4 import BeautifulSoup # Python library for pulling data out of HTML and XML files\n",
    "from requests import get # standard for making HTTP requests in Python\n",
    "import pandas as pd # library written for data manipulation and analysis\n",
    "import sys, time #  System-specific parameters and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Empty lists for Alcholic Anonymous News Articles parameters data to be extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines, descriptions, dates, authors, news, keywords, summaries, urls = [], [], [], [], [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the total no.of.pages by total no.of articles from google search resultsÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'Alcoholics Anonymous site:www.indiatoday.in'\n",
    "\n",
    "url = 'https://www.google.com/search?q=' + '+'.join(keyword.split())\n",
    "\n",
    "options = Options()\n",
    "options.headless = True\n",
    "browser = webdriver.Chrome(options=options)\n",
    "browser.get(url)\n",
    "\n",
    "page = browser.page_source\n",
    "soup = BeautifulSoup(page, 'lxml')\n",
    "max_pages = round([int(s) for s in soup.select_one('#resultStats').text.split() if s.isdigit()][0]/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterates max_pages value through while loop. Scraping the Articles urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 : 11\r"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        index +=1\n",
    "        page = browser.page_source\n",
    "        soup = BeautifulSoup(page, 'lxml')\n",
    "        linky = [soup.select('.r')[i].a['href'] for i in range(len(soup.select('.r')))]\n",
    "        urls.extend(linky)\n",
    "        if index == max_pages:\n",
    "            break\n",
    "        browser.find_element_by_xpath('//*[@id=\"pnnext\"]/span[2]').click()\n",
    "        time.sleep(2)\n",
    "        sys.stdout.write('\\r' + str(index) + ' : ' + str(max_pages) + '\\r')\n",
    "        sys.stdout.flush()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To remove duplicates urls entries in the list by executing below line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "urls = list(dict.fromkeys(urls))\n",
    "print(len(urls), type(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterates urls through for loop. Scraping the Articles with above parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 3sndiatoday.in/magazine/leisure/story/20180129-food-writing-recipe-books-the-suriani-kitchen-flavour-of-spice-1149458-2018-01-19charges-769857-2013-11-3001-06-04\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for index, url in enumerate(urls):\n",
    "    try:\n",
    "        # Parse the url to NewsPlease \n",
    "        soup = BeautifulSoup(get(url).text, 'lxml')\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        article.nlp()\n",
    "        \n",
    "        # Extracts the Headlines\n",
    "        try:\n",
    "            try:\n",
    "                headlines.append(article.title.strip())\n",
    "            except:\n",
    "                headlines.append(soup.select_one('meta[property=\"og:title\"]')['content'].strip())\n",
    "        except:\n",
    "            headlines.append(None)\n",
    "            \n",
    "        # Extracts the Descriptions    \n",
    "        try:\n",
    "            try:\n",
    "                descriptions.append(soup.select_one('meta[property=\"og:description\"]')['content'].strip().replace('\\n', ' '))\n",
    "            except:\n",
    "                descriptions.append(article.meta_description.strip())\n",
    "        except:\n",
    "            descriptions.append(None)\n",
    "            \n",
    "        # Extracts the Authors\n",
    "        try:\n",
    "            try:\n",
    "                authors.append(soup.select_one('meta[name=\"author\"]')['content'].strip())\n",
    "            except:\n",
    "                authors.append(article.authors.strip())\n",
    "        except:\n",
    "            authors.append(None)\n",
    "        \n",
    "        # Extracts the published dates\n",
    "        try:\n",
    "            try:\n",
    "                dates.append(soup.select_one('meta[property=\"article:published_time\"]')['content'].strip())\n",
    "            except:\n",
    "                dates.append(str(article.publish_date))\n",
    "        except:\n",
    "            dates.append(None)\n",
    "            \n",
    "        # Extracts the news articles\n",
    "        try:\n",
    "            try:\n",
    "                news.append(soup.select_one('.storyText').text.replace('\\n', '').strip())\n",
    "            except:\n",
    "                news.append(article.text.strip())\n",
    "        except:\n",
    "            news.append(None)\n",
    "            \n",
    "        # Extracts Keywords and Summaries    \n",
    "        try:\n",
    "            keywords.append(article.keywords)\n",
    "            summaries.append(article.summary)\n",
    "        except:\n",
    "            keywords.append(None)\n",
    "            summaries.append(None)\n",
    "            \n",
    "    except:\n",
    "        headlines.append(None)\n",
    "        descriptions.append(None)\n",
    "        authors.append(None)\n",
    "        dates.append(None)\n",
    "        news.append(None)\n",
    "        keywords.append(None)\n",
    "        summaries.append(None)\n",
    "\n",
    "    sys.stdout.write('\\r' + str(index) + ' : ' + str(url) + '\\r')\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Array Length of each list to create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 47 47 47 47 47 47 47\n"
     ]
    }
   ],
   "source": [
    "print(len(headlines), len(descriptions), len(authors), len(dates), len(news), len(keywords), len(summaries), len(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a csv file after checking array length and if to url error creates NaN values in the rows and dropping it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headlines</th>\n",
       "      <th>Descriptions</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Published_Dates</th>\n",
       "      <th>Articles</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the most successful ways of treating ad...</td>\n",
       "      <td>Today the T.T. Ranganathan Clinical Research C...</td>\n",
       "      <td>None</td>\n",
       "      <td>2014-01-27T16:19:47+05:30</td>\n",
       "      <td>Aversion therapy: effective Aversion therapy: ...</td>\n",
       "      <td>[centre, help, aa, sessions, treatment, proble...</td>\n",
       "      <td>It first admits patients to a nursing home for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Original 1939 Alcoholics Anonymous Manuscript ...</td>\n",
       "      <td>The original manuscript of Alcoholics Anonymou...</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-05-19 00:00:00</td>\n",
       "      <td>The original manuscript of Alcoholics Anonymou...</td>\n",
       "      <td>[million, wilson, sold, manuscript, sell, 1939...</td>\n",
       "      <td>The original manuscript of Alcoholics Anonymou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Demi Lovato left Met Gala to go to Alcoholics ...</td>\n",
       "      <td>Demi Lovato left Met Gala to go to Alcoholics ...</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-03-10T11:05:11+05:30</td>\n",
       "      <td>Los Angeles, Mar 10 (PTI) Demi Lovato has reca...</td>\n",
       "      <td>[left, met, diamonds, party, terrible, lovato,...</td>\n",
       "      <td>Los Angeles, Mar 10 (PTI) Demi Lovato has reca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Compounded by denial and loneliness, alcoholis...</td>\n",
       "      <td>Compounded by denial and a dearth of treatment...</td>\n",
       "      <td>None</td>\n",
       "      <td>2013-06-20T14:12:29+05:30</td>\n",
       "      <td>More than anything else, this letter from her ...</td>\n",
       "      <td>[problem, support, alcoholic, denial, treatmen...</td>\n",
       "      <td>Now, there are 30 to 35 every month and the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is the 12-step programme any good for sex addi...</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>2010-09-08T15:11:36+05:30</td>\n",
       "      <td>Vaccine benefits elude older women\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>[power, addiction, plan, problem, life, sexual...</td>\n",
       "      <td>This declaration appeared to be in the line of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Headlines  \\\n",
       "0  One of the most successful ways of treating ad...   \n",
       "1  Original 1939 Alcoholics Anonymous Manuscript ...   \n",
       "2  Demi Lovato left Met Gala to go to Alcoholics ...   \n",
       "3  Compounded by denial and loneliness, alcoholis...   \n",
       "4  Is the 12-step programme any good for sex addi...   \n",
       "\n",
       "                                        Descriptions Authors  \\\n",
       "0  Today the T.T. Ranganathan Clinical Research C...    None   \n",
       "1  The original manuscript of Alcoholics Anonymou...    None   \n",
       "2  Demi Lovato left Met Gala to go to Alcoholics ...    None   \n",
       "3  Compounded by denial and a dearth of treatment...    None   \n",
       "4                                                       None   \n",
       "\n",
       "             Published_Dates  \\\n",
       "0  2014-01-27T16:19:47+05:30   \n",
       "1        2017-05-19 00:00:00   \n",
       "2  2018-03-10T11:05:11+05:30   \n",
       "3  2013-06-20T14:12:29+05:30   \n",
       "4  2010-09-08T15:11:36+05:30   \n",
       "\n",
       "                                            Articles  \\\n",
       "0  Aversion therapy: effective Aversion therapy: ...   \n",
       "1  The original manuscript of Alcoholics Anonymou...   \n",
       "2  Los Angeles, Mar 10 (PTI) Demi Lovato has reca...   \n",
       "3  More than anything else, this letter from her ...   \n",
       "4  Vaccine benefits elude older women\\n\\n\\n\\n\\n\\n...   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  [centre, help, aa, sessions, treatment, proble...   \n",
       "1  [million, wilson, sold, manuscript, sell, 1939...   \n",
       "2  [left, met, diamonds, party, terrible, lovato,...   \n",
       "3  [problem, support, alcoholic, denial, treatmen...   \n",
       "4  [power, addiction, plan, problem, life, sexual...   \n",
       "\n",
       "                                           Summaries  \n",
       "0  It first admits patients to a nursing home for...  \n",
       "1  The original manuscript of Alcoholics Anonymou...  \n",
       "2  Los Angeles, Mar 10 (PTI) Demi Lovato has reca...  \n",
       "3  Now, there are 30 to 35 every month and the in...  \n",
       "4  This declaration appeared to be in the line of...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl = pd.DataFrame({'Headlines' : headlines,\n",
    "                    'Descriptions' : descriptions,\n",
    "                    'Authors' : authors,\n",
    "                    'Published_Dates' : dates, \n",
    "                    'Articles' : news,\n",
    "                    'Keywords' : keywords,\n",
    "                    'Summaries' : summaries,})\n",
    "\n",
    "tbl.to_csv('India_Today.csv', index=False)\n",
    "tbl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
